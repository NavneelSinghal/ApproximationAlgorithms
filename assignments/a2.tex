\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{thmtools}
\usepackage{enumitem}
\usepackage[framemethod=TikZ]{mdframed}

\usepackage{xpatch}

\usepackage{boites}
\makeatletter
\xpatchcmd{\endmdframed}
{\aftergroup\endmdf@trivlist\color@endgroup}
{\endmdf@trivlist\color@endgroup\@doendpe}
{}{}
\makeatother

%\usepackage[poster]{tcolorbox}
%\allowdisplaybreaks
%\sloppy

\usepackage[many]{tcolorbox}

\xpatchcmd{\proof}{\itshape}{\bfseries\itshape}{}{}

% to set box separation
\setlength{\fboxsep}{0.8em}
\def\breakboxskip{7pt}
\def\breakboxparindent{0em}

\newenvironment{proof}{\begin{breakbox}\textit{Proof.}}{\hfill$\square$\end{breakbox}}
\newenvironment{ans}{\begin{breakbox}\textit{Answer.}}{\end{breakbox}}
\newenvironment{soln}{\begin{breakbox}\textit{Solution.}}{\end{breakbox}}

% \tcolorboxenvironment{proof}{
%     blanker,
%     before skip=\topsep,
%     after skip=\topsep,
%     borderline={0.4pt}{0.4pt}{black},
%     breakable,
%     left=12pt,
%     right=12pt,
%     top=12pt,
%     bottom=12pt,
% }
%
% \tcolorboxenvironment{ans}{
%     blanker,
%     before skip=\topsep,
%     after skip=\topsep,
%     borderline={0.4pt}{0.4pt}{black},
%     breakable,
%     left=12pt,
%     right=12pt,
% }

\mdfdefinestyle{enclosed}{
    linecolor=black
    ,backgroundcolor=none
    ,apptotikzsetting={\tikzset{mdfbackground/.append style={fill=gray!100,fill opacity=.3}}}
    ,frametitlefont=\sffamily\bfseries\color{black}
    ,splittopskip=.5cm
    ,frametitlebelowskip=.0cm
    ,topline=true
    ,bottomline=true
    ,rightline=true
    ,leftline=true
    ,leftmargin=0.01cm
    ,linewidth=0.02cm
    ,skipabove=0.01cm
    ,innerbottommargin=0.1cm
    ,skipbelow=0.1cm
}

\mdfsetup{%
    middlelinecolor=black,
    middlelinewidth=1pt,
roundcorner=4pt}

\setlength{\parindent}{0pt}

\mdtheorem[style=enclosed]{theorem}{Theorem}
%\mdtheorem[style=enclosed]{lemma}{Lemma}[theorem]
%\mdtheorem[style=enclosed]{claim}{Claim}[theorem]
\mdtheorem[style=enclosed]{lemma}{Lemma}[section]
\mdtheorem[style=enclosed]{claim}{Claim}[section]
\mdtheorem[style=enclosed]{ques}{Question}
\mdtheorem[style=enclosed]{defn}{Definition}
\mdtheorem[style=enclosed]{notn}{Notation}
\mdtheorem[style=enclosed]{obs}{Observation}
\mdtheorem[style=enclosed]{eg}{Example}
\mdtheorem[style=enclosed]{cor}{Corollary}
\mdtheorem[style=enclosed]{note}{Note}

% \let\thetheorem=\relax
% \let\thelemma=\relax
% \let\theclaim=\relax
% \let\theques=\relax
% \let\thedefn=\relax
% \let\thenotn=\relax
% \let\theobs=\relax
% \let\thecor=\relax
% \let\thenote=\relax

% \renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\nl}{\vspace{0.2cm}\\}
\newcommand{\ol}{\overline}
\newcommand{\eps}{\varepsilon}
\newcommand{\mc}{\mathcal}
\newcommand{\mi}{\mathit}
\newcommand{\mf}{\mathbf}
\newcommand{\mb}{\mathbb}
\newcommand{\R}{\mathbb{R}}
\newcommand{\OPT}{\mathbf{OPT}}
\newcommand{\ALG}{\mathbf{ALG}}
\renewcommand{\L}{\mc{L}}
\newcommand{\changesto}{\vdash}
\newcommand\Vtextvisiblespace[1][.3em]{%
    \mbox{\kern.06em\vrule height.3ex}%
    \vbox{\hrule width#1}%
    \hbox{\vrule height.3ex}
}
\newcommand{\blank}{{\Vtextvisiblespace[0.7em]}}
\newcommand{\leftend}{\triangleright}
\newcommand{\comp}{\overline}

\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}
\pdfsuppresswarningpagegroup=1

\title{\textbf{Assignment 2}}
\author{Navneel Singhal}
\date{2018CS10360}

\begin{document}
\maketitle
\tableofcontents

\newpage


%\begin{algorithmic}[1]
%    \Function{ApproxSetMulticover}{$U = \{e_1, \ldots, e_n\}$, $\mathcal{S} = \{S_1, \ldots, S_m\}$, $req[1\ldots n]$}
%        \State Let $X \gets U$ \Comment The set of elements whose requirements haven't been met so far
%        \State Let $A$ be a multiset of subsets of $U$, initialized to $\emptyset$.
%        \State Let $f[1\ldots n]$ be an array initialized to all 0s
%        \Comment $f[i]$ will be the number of copies of $e_i$ covered so far
%        \While{$X$ is non-empty}
%            \State Let $S_i$ be a set that covers the most number of elements in $X$
%            \State Insert $S_i$ into $A$
%            \For{$e_j$ in $S_i$}
%                \State $f[j] \gets f[j] + 1$
%                \If{$f[j] = req[j]$}
%                    \State Remove $e_j$ from $X$.
%                \EndIf
%            \EndFor
%        \EndWhile
%        \State \Return $A$
%    \EndFunction
%\end{algorithmic}


\section{Problem 1}
\subsection{Statement}
Consider the following scheduling problem: there are $n$ jobs to be scheduled on a single machine, where each job $j$ has a processing time $p_j$, a weight $w_j$, and a due date $d_j$, $j = 1,
\ldots, n$. The objective is to schedule the jobs so as to maximise the total weight of jobs that complete by their due date. First prove that there always exists an optimal schedule in which
all on-time jobs complete before all late jobs and the on-time jobs complete in the earliest due date order. Use this structural result to show how to solve this problem using Dynamic
Programming in $O(nW)$ time where $W = \sum_j w_j$. Now use this result to derive a fully polynomial-time approximation scheme.
\subsection{Solution}
\begin{claim}
    There exists an optimal schedule in which all on-time jobs complete before all late jobs and the on-time jobs complete in the earliest due date order.
\end{claim}
\begin{proof}
    For the first part, let $S$ be an optimal schedule with jobs $s_j$ in increasing order of starting/ending time (both these orders are equivalent since scheduling is done on a single machine
    without overlaps).\nl
    If for some $i < j$, $s_i$ is a late job and $s_j$ is an on-time job, we call $(i, j)$ to be an \emph{inversion}.\nl
    Among all schedules, wlog that $S$ is one with the minimum number of inversions. If the number of inversions in $S$ is not $0$, then there must be at least one inversion in $S$. It then follows that for some index $i$, there is an inversion of the
    form $(i, i + 1)$ (since if there wasn't, then either both $i$ and $i + 1$ would have been late or on-time, or $i$ would have been on-time and $i + 1$ would have been late, in which case there
    is no inversion by induction on the index $i$).\nl
    Consider the order $S' = s_1, \ldots, s_{i-1}, s_{i+1}, s_i, s_{i+2}\ldots, s_n$ (the prefix and suffix may be empty) which we get from swapping $s_i$ and $s_{i+1}$ in $S$.\nl
    Note that this is also an optimal schedule, since we are moving an on-time job to earlier, so it stays on-time in the new schedule as well, and the late job remains a late job since we're pushing
    it to a later time.\nl
    Note that after this swap, the only pair of indices whose relative order changes is $(i, i + 1)$. So the number of inversions reduces by strictly 1, but this is a contradiction since $S$ was
    an optimal schedule with the smallest number of inversions.\nl
    For the second part, let $S$ be an optimal schedule with all the on-time jobs completing before all the late jobs. We call a pair of indices $(i, j)$ a date-inversion if $i < j$ and
    $d_{s_i} > d_{s_j}$, and $s_i, s_j$ are both on-time jobs. Again, as before, suppose that the least number of date inversions for any optimal schedule with on-time jobs before late jobs is non-zero. Then there exists a date-inversion of
    the form $(i, i + 1)$ as well. Suppose the time at which job $s_i$ started was $t$. Then we have $t + p_{s_i} \le d_{s_i}$ and $t + p_{s_i} + p_{s_{i+1}} \le d_{s_{i+1}}$. Upon swapping
    jobs $s_i$ and $s_{i+1}$ in the schedule, note that $t + p_{s_{i+1}} \le t + p_{s_i} + p_{s_{i+1}} \le d_{s_{i+1}}$ and $t + p_{s_i} + p_{s_{i+1}} \le d_{s_{i+1}} < d_{s_i}$, the new schedule
    so formed is also a valid (and optimal) schedule. Hence we can reduce the number of date-inversions in this case as well, which is a contradiction.\nl
    This proves the claim.
\end{proof}

% Algorithm
%% wlog assume p[j] <= d[j] (else we can never do this job)
%% dp[i][j] = max weight of all on-time jobs that finish on or before time j taken from the first i elements
%% if j < d[i], dp[i][j] = max(dp[i][j - 1], dp[i - 1][j])
%% else dp[i][j] = max(dp[i][j - 1], dp[i - 1][j], w[i] + dp[i - 1][j - p[i]])
% this is kind of bad since it depends on max due date

%% dp[i][j] = min time taken to get a cost >= j from the first i elements
%% choose ith element or don't
%% -> if chosen and dp[i - 1][max(0, j - w[i])] + p[i] <= d[i], then dp[i - 1][max(0, j - w[i])] + p[i]
%% -> if not chosen, then dp[i - 1][j]

Using this result, we come up with the following algorithm to compute the maximum total weight of jobs we can get; it is straightforward to restore the schedule using the dynamic programming
table constructed in the solution (note that we can remove all jobs which take more time than their due date, so we can also assume $p_i \le d_i$):

\begin{algorithmic}[1]
    \Function{Schedule}{$n$, $p[1 \ldots n]$, $w[1 \ldots n]$, $d[1 \ldots n]$}
        \State Let $W = \sum_{i = 1}^n w[i]$
        \State Let $dp[0 \ldots n][0 \ldots W]$ be a matrix initialized to all $\infty$s
        \Comment{$dp[i][j]$ = minimum time taken to get a total weight of at least $j$ from the first $i$ elements}
        \State $dp[0][0] \gets 0$
        \For{$i = 1 \ldots n$}
            \For{$j = 0 \ldots W$}
                \State $dp[i][j] \gets dp[i - 1][j]$
                \If{$dp[i - 1][\max(0, j - w[i])] + p[i] \le d[i]$}
                    \State $dp[i][j] \gets \min(dp[i][j], dp[i - 1][\max(0, j - w[i])] + p[i])$
                \EndIf
            \EndFor
        \EndFor
        \State \Return the largest $w$ such that $dp[n][w] \ne \infty$ \Comment{To restore the schedule from this, we can reconstruct our choices using the $dp$-matrix}
    \EndFunction
\end{algorithmic}
For a proof of correctness, note that we have the following induction to show that $dp[i][j]$ indeed is the quantity mentioned in the comment.
\begin{proof}
    \begin{enumerate}
        \item Base case: $i = 0$: In this case, we can only get a weight of $0$, so $dp[0][0] = 0$ and $dp[0][w] = \infty$ for $w > 0$, which matches.
        \item Inductive step: $i > 0$. We have only two choices:
            \begin{itemize}
                \item In the case where we choose the $i^\mathrm{th}$ element as an on-time job, we need to check if the minimum time taken to finish this job while having a total weight at
                    least $j$ is still $\le$ its due date (else this won't be an on-time job), and this condition is sufficient as well. In the case that this condition is true, we only need to
                    minimum time taken to complete jobs with total weight $\ge j - w[i]$ from the first $i - 1$ elements. Upon noting that time taken can't be negative, we get the transition as in
                    the algorithm by the inductive hypothesis.
                \item In the case where we don't choose it, we can simply use the minimum time taken for some of the first $i - 1$ jobs to reach a total weight of $j$ (infinite if impossible),
                    and this leads to the transition as in the algorithm by the inductive hypothesis.
            \end{itemize}
    \end{enumerate}
    Hence this completes the proof.
\end{proof}
Now the answer is the largest $w$ which doesn't take infinite time to be achieved, and we are done. The time taken by the algorithm is $O(nW)$.\nl

The approximation algorithm for a $(1 - \eps)$ approximation is as follows:
\begin{enumerate}
    \item Let $w'$ be the maximum weight of a job.
    \item Replace $w_i$ by $\lfloor\frac{nw_i}{w'\eps}\rfloor$
    \item Output the schedule returned by the previous algorithm on this instance.
\end{enumerate}
Note that this schedule is valid, and since we didn't change the processing time or the due dates, the jobs which are on-time in this solution also remain on-time in the solution corresponding to
the original problem.\nl

%% Approximation - guarantee
\begin{claim}
    This is a $1 - \eps$ approximation.
\end{claim}
\begin{proof}
    Let $\OPT$ be the ordering of the on-time jobs in an optimal schedule for the original problem. Let $\OPT'$ be defined similarly for the modified problem that we return.\nl
    Then we have the following:
    \begin{align*}
        \sum_{j \in \OPT'} w_j &\ge \frac{w' \eps}{n} \sum_{j \in \OPT'} \left\lfloor\frac{nw_j}{w'\eps}\right\rfloor\\
                        &\ge \frac{w' \eps}{n} \sum_{j \in \OPT} \left\lfloor\frac{nw_j}{w'\eps}\right\rfloor\\
                        &\ge \frac{w' \eps}{n} \sum_{j \in \OPT} \left(\frac{nw_j}{w'\eps} - 1\right)\\
                        &= \sum_{j \in \OPT} w_j - w' \eps \frac{|\OPT|}{n}\\
                        &\ge \sum_{j \in \OPT} w_j - w' \eps
    \end{align*}
    Here the first inequality comes from rounding, the second from the fact that $\OPT'$ is an optimal solution to the modified problem, the third from rounding again, and the last one from
    the fact that we have at most $n$ jobs in $\OPT$.\\
    Now note that we can always take the maximum element as an on-time job and schedule everything else to be a late job (this is possible since we assume $p_i \le w_i$ for all $i$). This
    shows that $\sum_{j \in \OPT} w_j \ge w'$. Plugging this into the above inequality, we have:
    $\sum_{j \in \OPT'} w_j \ge (1 - \eps) \sum_{j \in \OPT} w_j$, and we are done.
\end{proof}

\begin{claim}
    The time complexity is $O(n^3 / \eps)$ (and hence this algorithm is a FPTAS).
\end{claim}
\begin{proof}
    The largest weight is reduced to $\lfloor\frac{n}{\eps}\rfloor$, and since the mapping is a non-decreasing function, all weights are at most $\frac{n}{\eps}$. So $W \in O(\frac{n^2}{\eps})$,
    and the algorithm becomes $O(n^3 / \eps)$, and we are done.
\end{proof}

\newpage

\section{Problem 2}
\subsection{Statement}
Consider the following scheduling problem: there are $n$ jobs to be scheduled on a constant number of machines $m$, where each job $j$ has a processing time $p_j$, and a weight $w_j$, $j = 1, \ldots, n$.
Once started, each job must be processed to completion on that machine without interruption. For a given schedule, let $C_j$ denote the completion time of job $j$, $j = 1, \ldots, n$ and the
objective is to minimise $\sum_j w_j C_j$ over all possible schedules. First show that there exists an optimal schedule where, for each machine, the jobs are scheduled in non-decreasing
$p_j/w_j$ order. Then use this property to derive a dynamic programming algorithm that can be used to obtain a fully polynomial-time approximation scheme.
\subsection{Solution}
I will assume that the phrase ``constant number of machines'' implies that the number of machines is not in the input, but rather a part of the problem description.\nl
\begin{claim}
    Given a machine, there is an optimal schedule where the jobs on it are scheduled in non-decreasing $p_j / w_j$ order.
\end{claim}
\begin{proof}
    Suppose not. Then any optimal schedule with the least number of inversions (in the array of $p_j / w_j$) has at least one inversion. This also implies that there is an inversion where the
    relevant indices are adjacent (else by induction, the array of $p_j / w_j$ induced by this ordering is sorted, which is a contradiction).\nl
    Suppose that there is an inversion at indices $i, i + 1$, i.e., if job $k$ is at the $i^\mathrm{th}$ position, and job $l$ is at the $(i + 1)^\mathrm{th}$ position, then we have $p_k / w_k > p_l
    / w_l$.
    Consider what happens when we swap their positions. Since the set of jobs in the prefix of size $j$ where $j < i$ or $j > i + 1$ doesn't change, the change in the objective function is only
    due to the terms arising from indices $i$ and $i + 1$.\nl
    The change is $-((C + p_k) \cdot w_k + (C + p_k + p_l) \cdot w_l) + ((C + p_l) \cdot w_l + (C + p_l + p_k) \cdot w_k) = - p_k \cdot w_l + p_l \cdot w_k = w_k w_l \cdot \left(-\frac{p_k}{w_k}
        +\frac{p_l}{w_l}\right) \le 0$.\nl
        Hence, swapping leads to one less inversion (since there are no extra inversions added/removed by swapping consecutive positions) and doesn't increase the objective function. This is
        a contradiction to the fact that we have at least one inversion in any optimal schedule, which completes the proof.
\end{proof}
Now we will show a dynamic programming algorithm that can find an optimal solution using this lemma. What follows will work for processing times being real numbers, but we will be able to give a
good bound on time complexity only when the processing times are integers (or rationals). For integers, rather than a red-black tree, we can use an $m-$dimensional array to make the
implementation faster.
\begin{algorithmic}[1]
    \Function{Schedule}{$n$, $p[1 \ldots n]$, $w[1 \ldots n]$}
        \State Let $id[1 \ldots n]$ be the order of indices sorted by $p[j] / w[j]$
        \State Let $T = \sum_{i = 1}^n p[i]$
        \State Let $dp[0 \ldots n]$ be an array of red-black trees supporting key-value mappings initialized to all empty trees.
        \State \Comment{$dp[i]$ consists of key-value pairs where the key is the $m$-tuple of completion times of each machine in sorted order, and the value is the minimum objective value
        corresponding to any configuration with jobs in $id[1 \ldots i]$ with that key. Ordering is lexicographical.}
        \State Let $parent[1 \ldots n]$ be an array of red-black trees supporting key-value mappings initialized to all empty trees.
        \State \Comment{$parent[i]$ consists of key-value pairs where the key is the $m-$tuple of completion times of each machine in sorted order, and the value is the tuple of completion times, which when
        appended by the $id[i]^\mathrm{th}$ job, leads to the key. Ordering is lexicographical here again}
        \State \Comment{We will represent these balanced binary search trees as sets.}
        \State $dp[0] \gets \{ ((0, \ldots, 0), 0) \}$
        \For{$i = 1 \ldots n$}
            \For{$((t_1, t_2, \ldots, t_m), v)$ in $dp[i - 1]$}
                \For{$j = 1 \ldots m$}
                    \State Let $(t_1', \ldots, t_m') = (t_1, \ldots, t_m)$
                    \State $t_j' \gets t_j + p[id[i]]$
                    \State Let $v' = v + w[id[i]] \cdot t_j'$
                    \State Sort the tuple $(t_1', \ldots, t_m')$
                    \If{$(t_1', \ldots, t_m') \not\in dp[i]$}
                        \State Insert $((t_1', \ldots, t_m'), v')$ into $dp[i]$
                        \State Insert $((t_1', \ldots, t_m'), (t_1, \ldots, t_m))$ into $parent[i]$.
                    \ElsIf{the value of the key $(t_1', \ldots, t_m')$ in $dp[i]$ is $> v'$}
                        \State Update its value to $v'$
                        \State Update the value of the key $(t_1', \ldots, t_m')$ in $parent[i]$ to $(t_1, \ldots, t_m)$.
                    \EndIf
                \EndFor
            \EndFor
        \EndFor
        %\State Let $S[1 \ldots n]$ be an array of sets, $S[i]$ being the set of jobs assigned to machine $i$ so far. These are all initialized to empty sets.
        \State Let $l$ be an empty list of $m$-tuples.
        \State Let $(t_1, \ldots, t_m)$ be a key in $dp[n]$ with the largest value.
        \For{$i = n \ldots 1$}
            \State Let $v$ be the value of $(t_1, \ldots, t_m)$ in $dp[i]$.
            \For{$j = 1 \ldots m$}
                \State Let $(t_1', \ldots, t_m')$ be the tuple we get after subtracting $p[id[i]]$ from $t_j$ in $(t_1, \ldots, t_m)$.
                \State Let $v' = v - w[id[i]] \cdot t_j$.
                \State Sort this tuple.
                \If{the value of $(t_1', ..., t_m')$ in $dp[i - 1]$ is $v'$}
                    \State Add $(t_1, \ldots, t_m)$ to the head of $l$.
                    \State Replace $(t_1, \ldots, t_m)$ with $(t_1', \ldots, t_m')$.
                    \State Continue
                \EndIf
            \EndFor
        \EndFor
        \State \Comment{The $i^\mathrm{th}$ tuple in $l$ is the tuple of sorted completion times of all machines in an optimal solution when considering only jobs numbered $id[1 \ldots i]$}
        \State Assign jobs to the correct machines by simulating the addition and sorting procedure by maintaining a permutation of machine indices at each step.
        \State Return the solution so formed.
    \EndFunction
\end{algorithmic}
For a brief proof of correctness, note the following:
\begin{enumerate}
    \item The correctness of the order of processing the jobs lies in the fact that in the kind of optimal solution we are interested in, for all machines, there is a maximal prefix which has jobs
        with numbers among $id[1 \ldots i]$ only.
    \item The proof of correctness of the state transitions in $dp$ is straightforward, and can be done using an induction that relies upon the fact that all configurations of
        machines come from some previous configuration by appending a job to some prefix, and that the increase in objective function in appending a job with processing time $t$ and
        weight $w$ to a machine with completion time $C$ is precisely $w \cdot (C + t)$, and that we are essentially finding the minimum of the objective function over all `optimal' ways (for jobs
        $id[1 \ldots i - 1]$) to reach to a candidate optimal parent configuration and then transition to a new configuration.
    \item The part of the program from lines 27 to 43 is merely to reconstruct the assignment of the jobs to machines.
\end{enumerate}

For now, we will assume that the times $p[i]$ are all integers. Let $T = \sum_{i = 1}^n p[i]$. We show that the time taken by the algorithm is polynomial in $n$ and $T$.\nl
Firstly note that for any $dp[i]$, since all machines have sorted tuples of integers between $0$ to $T$, and each tuple is associated with at most one value, and the sum of tuples is constant (and
equals $\sum_{j = 1}^i p[id[i]]$), the number of elements in $dp[i]$ is $O(T^{m - 1})$ (in fact it would be better than this in practice, and would be at most $O(\binom{T + m - 2}{m - 1})$ which is asymptotically
the same but has a better constant factor).
The time complexity is strictly dominated by the loop that fills in $dp[1 \ldots n]$. Searching/inserting/deletion in a red-black tree takes $O(m \log |dp[i]|) \in O(m^2 \log T)$ time. The rest of
the operations in a single iteration of the loop at line 12 take a total of $O(m \log m)$ time.\nl
Since there are $m \cdot |dp[i - 1]|$ iterations for each $i$, one iteration of the loop at line 10 takes $O(m^3 T^{m - 1} \log T)$ time. The time taken over all loops is thus $O(n m^3 T^{m - 1}
\log T)$, which is polynomial in $n$ and $T$, keeping in mind that $m$ is a problem-specified constant.\nl

For this problem, my rounding ideas didn't work out (since the error term is proportional to $T \sum_i w_i$ which might be much larger in order of magnitude than $\OPT$). However, I tried to scale and
round values of each solution into certain intervals similar to those made in bin-packing to get a not-too-large deviation from $\OPT$. For that, it is necessary to bound the number of keys we
can have for each value, else the algorithm still remains pseudo-polynomial in the input.\nl
Now we need to construct a FPTAS based on this solution. Note that this solution works for $m \le M = 2$, I wasn't able to solve for $m > M$.\nl
Firstly, we claim that if $m \le M$, it is possible to impose a total-order on tuples for a given value.\nl
For that, we first claim that for any partition of the remaining jobs $A_1, \ldots, A_m$ into $m$ sets, with the total time of each set on a single machine being $a_1, \ldots, a_m$
respectively, it is optimal to assign the highest $a_i$ to the lowest $t_j$ and so on.\nl
For a proof, note that the final answer is $O + \sum a_i + \sum_{(i, j) \in S} a_i \cdot t_j$ where $S$ is the set of pairs whose first component is the index of the set $A_i$ and second component is
the index of the machine to which it is assigned. So by the rearrangement inequality, the result follows.\nl
Consider two tuples $(t_1, t_2, \ldots, t_m)$ and $(t_1', t_2', \ldots, t_m')$ both in sorted order. Suppose $a_1 \ge \ldots \ge a_m$, without loss of generality. Then the objective value taking
the first tuple is $\sum_i a_i t_i$ and that for the second tuple is $\sum_i t_i'$. Note that $\sum_i a_i t_i = \sum_i \left((a_i - a_{i+1}) \cdot \sum_{j = 1}^i t_j\right)$, where $a_{m+1}
= 0$.\nl
Note that $a_i - a_{i+1} \ge 0$ for all $i$. So in order to claim that the first tuple is better than the second tuple, it is sufficient to say that $\sum_{j = 1}^i t_j \ge \sum_{j = 1}^i t_j'$
for all $i$. Since the tuple sums are the same, by subtracting this inequality from the tuple sums, it is sufficient to check if the second tuple majorizes the first tuple (when reversed).\nl
For $m = 2$ this is clear, since we need to only check if $t_2 \le t_2'$ and $t_1 + t_2 = t_1' + t_2'$, which is equivalent to a lexicographical ordering on the original tuples. For $m = 3$, this
might not be possible since the tuples $(6, 3, 2)$ and $(5, 5, 1)$ are not comparable under the majorization relation.\nl
The case of $m = 1$ was settled earlier. So for $m = 2$, after we have computed for the answer for all tuples, we can remove all tuples other than the lexicographically smallest tuple for each
value, and get one tuple per value.\nl
Then ...


%For these purposes, we will first remove all jobs that have $w = 0$, since they can be put at the end of any machine without any increase in objective function value. Now we will rescale all
%$w_i$'s by the same constant factor so that $w_i \ge 1$ for all $i$. Note that after this rescaling, the input size changes to at most a quadratic function of what it was before, so if we have a
%FPTAS for this instance, we also have a FPTAS for the original instance.\nl
%For getting an FPTAS, we scale and round $p[i] \to \left\lfloor\frac{n \cdot p[i]}{T\eps}\right\rfloor$.\nl


\newpage

\section{Problem 3}
\subsection{Statement}
In the \emph{directed Steiner tree} problem, we are given as input a directed graph $G = (V, E)$, non-negative costs $c_{ij} \ge 0$ for arcs $(i, j) \in E$, a root vertex $r \in V$, and a set of
terminals $T \subseteq V$. The goal is to find a minimum-cost tree such that for each $i\in T$ there is a directed path from $r$ to $i$. It is NP-hard to approximate set-cover to a factor better than
$c\log n$, for some constant $c$, where $n$ is the number of elements. Use this fact to argue that for some constant $d$ there can be no $d \log |T|$-approximation algorithm for the directed Steiner tree problem, unless P = NP.
\subsection{Solution}
We claim that any $d < c$ satisfies this property.\nl
Suppose that there is a $d \log |T|$-approximation algorithm for the directed Steiner tree problem. We shall show that such an algorithm will directly lead to a $d \log n$ algorithm for an instance of
set cover with $m$ sets $S_1, \ldots, S_m$ and $n$ elements $e_1, \ldots, e_n$.
\begin{enumerate}
    \item Construct nodes $v_0$, $v_1, \ldots, v_m$ and $u_1, \ldots, u_n$ in a graph $G$.
    \item Join $v_0$ to each $v_i$ with an edge of cost $cost(S_i)$.
    \item For a set $S_i = \{e_{j_1}, \ldots, e_{j_k}\}$, join $v_i$ to $u_{j_1}, \ldots, u_{j_k}$ each with edges of cost $0$.
    \item Solve the directed Steiner tree problem for the graph $G$ with the costs assigned as above, root vertex $v_0$ and the set of terminals being $\{u_1, \ldots, u_n\}$.
    \item For each edge of the form $(v_0, v_k)$, add set $S_k$ into the solution.
    \item Return the solution.
\end{enumerate}

\begin{claim}
    This solution is a valid set cover.
\end{claim}

\begin{proof}
    Suppose that this is not the case. Then there exists an element $e_i$ which is not covered by the chosen sets. The corresponding vertex $u_i$ is hence not reachable from the root, since any
    path from $v_0$ to $u_i$ should pass through a $v_j$ such that $e_i$ is in $S_j$, and had we chosen any of these, this would had implied that $S_j$ was in the solution, contradicting that
    $e_i$ is not covered. So $u_i$ is not reachable from $r$, which is a contradiction to the fact that the algorithm gets a minimum cost tree such that each vertex in the terminal set
    is reachable from the root $v_0$.
\end{proof}

\begin{claim}
    An optimal solution to the constructed instance of directed Steiner tree corresponds to an optimal solution to the associated set cover problem.
\end{claim}

\begin{proof}
    We claim that for each solution of set cover with cost $C$, we can construct a solution of this instance of directed Steiner tree with cost $C$ and vice versa.\nl
    For the first part, let $S_{i_1}, \ldots, S_{i_k}$ be the sets chosen for the set cover. Then add the edges $(v_0, v_{i_r})$ and the edges $(v_{i_r}, u_j)$ for each element $e_j$ in $S_{i_r}$.
    Note that this is a directed tree since it has no cycles. Now since for each $e_r$ there is a set $S_{i_j}$ such that $e_r \in S_{i_j}$ (by the definition of set cover), there is a path $v_0
    \to v_{i_j} \to u_r$ from $v_0$ to $u_r$ for each $r$. The cost of this solution is clearly $C$, since the edges between $v_i$'s and $u_j$'s has contribution $0$ to the overall cost, and the
    contribution of the edge $(v_0, v_{i_j})$ is $cost(S_{i_j})$, and summing it gives the conclusion.\nl
    For the second part, construct any solution for this instance of directed Steiner tree problem which has cost $C$. Construct the solution in the same way as done in the algorithm above. Since for
    every vertex $u_i$, there is a path from $v_0$ to $u_i$, there must be a vertex $v_j$ on this path, which corresponds to including the set $S_j$ in the solution. Hence the corresponding
    solution is a set cover instance. Now note that by a very similar argument as in the previous part, the solution has the same cost.\nl
    Hence this shows that the optimal solution to the constructed instance of the directed Steiner tree corresponds to an optimal solution to the associated set cover solution.
\end{proof}

\begin{claim}
    This solution is a $d \log n$ approximation to the set cover problem.
\end{claim}

\begin{proof}
    By the assumption on the algorithm used to solve the directed Steiner tree problem, we know that the cost of the solution to the constructed directed Steiner tree instance is at most $d \log n$ times the cost of
    the optimal solution to the constructed directed Steiner tree instance. By the claim above, and the claim about the cost of the associated set cover solution having the same cost as the
    solution to the directed Steiner tree instance, we get the conclusion.
\end{proof}

However, since this approximation algorithm is a polynomial time algorithm, and it is NP-hard to approximate set cover to a factor better than $c \log n$, this shows that P = NP.
\newpage

\section{Problem 4}
\subsection{Statement}
The \emph{$k$-suppliers} problem is similar to the $k$-center problem discussed in class. The input to the problem is a positive integer $k$, and a metric on a set of points $V$, $|V| = n$. However,
now the points are partitioned into suppliers $F \subseteq V$ and customers $C = V \setminus F$. The goal is to pick $k$ suppliers such that the maximum distance between a customer and its nearest picked
supplier is minimized. In other words, we wish to find $S \subseteq F$, $|S| \le k$ that minimizes $\max_{j \in C} d(j, S)$ where $d(j, S)$ is the distance between $j$ and the nearest point in $S$.
Give a $3$-approximation algorithm for the $k$-suppliers problem.
\subsection{Solution}
The solution will closely mirror the one for the $k$-center problem as done in class.\nl
Our algorithm will use a predicate on a non-negative real $r$ as follows:
\begin{enumerate}
    \item Initialize the solution set to $\{\}$.
    \item While $C$ is non-empty:
        \begin{itemize}
            \item Pick a vertex $v$ from $C$, and consider the closest vertex $s$ to it in $F$. If the distance is more than $r$, return failure.
            \item Otherwise, consider all vertices in $C$ at a distance of at most $3r$ fom $s$, and remove them from $C$. Add $s$ to the solution set.
        \end{itemize}
    \item Return the solution set.
\end{enumerate}
\begin{claim}
    Let $r^*$ be the maximum distance between a customer and its nearest supplier in an optimal solution. If the algorithm returns failure or a solution with $> k$ suppliers, then $r < r^*$, otherwise it
    returns a solution with at most $k$ suppliers, with the maximum distance between a customer and a supplier being at most $3r$.
\end{claim}
\begin{proof}
    We break the proof into two parts. The second part is obvious, since the solution is valid, and the vertices removed all have a distance of $\le 3r$ from the supplier chosen in that step, so the
    closest supplier to them in the finally chosen set of suppliers has a distance at most $3r$ as well.\nl
    For the first part, there are two cases:
    \begin{enumerate}
        \item The algorithm returns a failure: In this case, there is a vertex which is at a distance of $> r$ from its nearest supplier. Hence $r^* > r$.
        \item The algorithm returns a solution with $> k$ suppliers: Suppose for the sake of contradiction that there is indeed a solution where the maximum distance between a customer and its
            nearest supplier is at most $r$, and the number of suppliers chosen is at most $k$. We show that the minimum number of suppliers that need to be chosen in any solution with maximum distance between a
        customer and supplier being $\le r$ is $> k$, which will lead to the desired contradiction.\nl
        Call any solution with the maximum distance between a customer and a supplier being $\le r$ an $r-$covered solution.\nl
        To this end, we show that for any $r$-covered solution $S$, we can assign a unique supplier belonging to this solution to each of the suppliers in the solution our algorithm returns.\nl
        Firstly note that at each step, we choose a different supplier, since we always clear out all vertices which are at a distance of at most $3r$ from that supplier, so any other
        vertex chosen will be at a distance of $> 3r$ from this supplier, and hence this supplier won't be chosen again.\nl
        Note that at each step, we choose a vertex $v$ corresponding to a new customer. In $S$, there must be a supplier $s'$ for this vertex. We will associate this supplier to the supplier
        $s$ chosen at this step.\nl
        Suppose that this supplier is chosen in two distinct steps (i.e., associated to two distinct suppliers picked by the algorithm), where the customers chosen were $u$ and $v$ respectively.
        Without loss of generality, suppose $v$ was picked before $u$. Then we have $d(u, s) \le d(u, s') + d(s', v) + d(v, s) \le 3r$, where the first two terms are bounded using the fact that
        $S$ is an $r$-covered solution, and the last term arises from the choice of $s$ when $v$ was picked. This inequality implies that $u$ is indeed removed when $s$ is chosen as the supplier,
        and this is a contradiction.\nl
        So we have shown a surjective partial function from the set of suppliers chosen in $S$ to the set of suppliers chosen by the algorithm. Hence the number of suppliers chosen in $S$ is
        at least the number of suppliers chosen by the algorithm, which is $> k$. Hence for any $r$-covered solution, the number of suppliers in it is more than $k$. Now note that by our
        assumption, there is a solution where the maximum distance between a customer and a supplier is at most $r$ and the number of suppliers chosen is at most $k$. This is a clear contradiction.\nl
        Now if $r \ge r^*$, any $r^*$-covered solution is also $r$-covered, hence it must have more than $k$ suppliers. This applies to the optimal solution of the original problem as well, so
        there should be no solution to the original problem in this case, which is a contradiction. Hence we must have $r < r^*$, since there exists at least one solution to the problem.
    \end{enumerate}
    This completes the proof of the first part of the claim as well, and we are done.
\end{proof}

Call the previous algorithm \textsc{SolveOrFail}$(r)$.\nl
The final algorithm becomes the following:
\begin{enumerate}
    \item For each possible distance $r$ between suppliers and customers (sorted in increasing order):
        \begin{itemize}
            \item Call \textsc{SolveOrFail}$(r)$.
            \item If it returns failure, continue.
            \item Else, return the solution it returns.
        \end{itemize}
\end{enumerate}

\begin{claim}
    The optimal solution of the problem is equal to one of the distances between the suppliers and the customers.
\end{claim}
\begin{proof}
    This is true since $r^*$ is in fact a distance between a customer and a supplier by definition in the previous claim.
\end{proof}
\begin{claim}
    The algorithm always returns a $3$-approximation for the $k$-suppliers problem.
\end{claim}
\begin{proof}
    Note that there is always an answer: just consider a single supplier; then the answer is in the set of distances between a customer and a supplier, and it is upper bounded by the maximum
    distance of this supplier from a customer.\nl
    Now consider the first $r$ where the algorithm doesn't give a failure. Since no previous distance returned a solution, we must have $r^* \ge r$. However, since the solution returned has the
    distance between any customer to its nearest supplier at most $3r$, the value of $\max_{j \in C} d(j, S)$ is $\le 3r \le 3r^*$, which is $3$ times the optimum. Thus, this is a $3$-approximation,
    as needed.
\end{proof}
\begin{claim}
    This is a polynomial-time algorithm.
\end{claim}
\begin{proof}
    Note that there are at most $O(n^2)$ distinct distances, and sorting would take roughly $O(n^2 \log n)$ (a better bound can be found by considering the size of $F$ etc., but the main goal is
    just to show that this is a polynomial time algorithm). At each step in the function \textsc{SolveOrFail}, we do $O(n)$ work to find the closest supplier to a customer, and remove all customers
    that are at a distance of at most $3r$ from this supplier. So overall, this algorithm is $O(n^3)$, which is polynomial time in the problem size.
\end{proof}

\end{document}
